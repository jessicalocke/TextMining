---
title: "Tidytext Text Mining using Harry Potter Novels"
author: "Jessica Locke"
date: "June 27, 2018"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(readr)
library(dplyr)
library(tidytext)
library(stringr)
library(jsonlite)
library(tidyr)
library(igraph)
library(ggraph)
library(widyr)
library(wordcloud)
library(reshape2)
library(topicmodels)
library(ggplot2)

```

Using tidy text principles outlined in "Text Mining with R" by Julia Silge and David Robinson
(https://www.tidytextmining.com/index.html)

#1 The tidy text format

##Using unnest_tokens to turn novels into a "tidy" format
```{r 1.2_load_data, echo = TRUE}

if (packageVersion("devtools") < 1.6) {
  install.packages("devtools")
}

#install the harrypotter package
devtools::install_github("bradleyboehmke/harrypotter")

library(harrypotter)

#Tidying text

#vector of book titles
titles <- c("Philosopher's Stone", "Chamber of Secrets", "Prisoner of Azkaban",
            "Goblet of Fire", "Order of the Phoenix", "Half-Blood Prince",
            "Deathly Hallows")

#list of all of the books
books <- list(philosophers_stone, chamber_of_secrets, prisoner_of_azkaban,
              goblet_of_fire, order_of_the_phoenix, half_blood_prince,
              deathly_hallows)

tidy_books <- tibble()

#for each book, turn into a tidy format (1 row per word) and append to tidy_books table
for(i in seq_along(titles)) {
  
  clean <- tibble(chapter = seq_along(books[[i]]),
                  text = books[[i]]) %>%
    unnest_tokens(word, text) %>%
    mutate(book = titles[i]) %>%
    select(book, everything())
  
  tidy_books <- rbind(tidy_books, clean)
  
  rm(clean)
}

#set factor to keep books in order of publication
tidy_books$book <- factor(tidy_books$book, levels = rev(titles))

#removing stopwords
data(stop_words)

tidy_books_clean <- tidy_books %>%
  anti_join(stop_words)

```

##Plotting the most common words in the novels

```{r 1.3_plot, echo = FALSE}

#top 20 words in whole series
tidy_books_clean %>%
  count(word, sort = TRUE) %>%
  top_n(20) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip() +
  ggtitle("Top 20 most common words in Harry Potter series")

#top 10 most common words in each book
tidy_books_clean %>%
  group_by(book) %>%
  count(word, sort = TRUE) %>%
  top_n(10) %>%
  ungroup() %>%
  mutate(book = factor(book, levels = titles),
         text_order = nrow(.):1) %>%
  ggplot(aes(reorder(word, text_order), n, fill = book)) +
  geom_bar(stat = "identity") +
  facet_wrap(~ book, scales = "free_y") +
  labs(x = "NULL", y = "Frequency") +
  coord_flip() +
  theme(legend.position="none") +
  ggtitle("Top 10 most common words in each novel")


```
##Comparing word frequencies across novels


```{r 1.4_book_vs_series_freq, echo = FALSE}
#calculate percent of word use across all novels
series_pct <- tidy_books_clean %>%
  count(word) %>%
  transmute(word, all_words = n / sum(n))

#calculate percent of word use within each novel
  ##total words in each novel
book_total_words <- tidy_books_clean %>%
  count(book) %>%
  rename(total_book_words = n)
  ##create df with word % within novel and within series
book_frequency <- tidy_books_clean %>%
  count(book, word) %>%
  left_join(book_total_words, by = c("book")) %>%
  mutate(book_words = n / total_book_words) %>%
  left_join(series_pct) %>%
  arrange(desc(book_words)) %>%
  ungroup()

#plot frequency of word use within a novel across series
ggplot(book_frequency, aes(x = book_words, y = all_words, color = abs(all_words - book_words))) +
  geom_abline(color = "gray40", lty = 2) +
  geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
  scale_x_log10(labels = scales::percent_format()) +
  scale_y_log10(labels = scales::percent_format()) +
  scale_color_gradient(limits = c(0, 0.001), low = "darkslategray4", high = "gray75") +
  facet_wrap(~ book, ncol = 2) +
  theme(legend.position="none") +
  labs(y = "Frequency in Harry Potter series", x = "Frequency per novel", title = "Word frequencies in series vs novels")
  
#word correlation across novels
book_frequency %>%
  group_by(book) %>%
  summarize(correlation = cor(book_words, all_words),
            p_value = cor.test(book_words, all_words)$p.value)

```

#Sentiment analysis

##Sentiment across duration of each novel

Using index to divide each novel into 500-word chunks (inclusive of stop words), we can plot the average sentiment of each chunk.
```{r 2.2_sentiment_within_book, echo = FALSE}


tidy_books %>%
  group_by(book) %>% 
  mutate(word_count = 1:n(),
         index = word_count %/% 500 + 1) %>% 
  inner_join(get_sentiments("bing")) %>%
  count(book, index = index , sentiment) %>%
  ungroup() %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative,
         book = factor(book, levels = titles)) %>%
  ggplot(aes(index, sentiment, fill = book)) +
  geom_bar(alpha = 0.5, stat = "identity", show.legend = FALSE) +
  facet_wrap(~ book, ncol = 2, scales = "free_x") +
  ggtitle("Average sentiment across novels")
```


##Comparing sentiment dictionaries

Focusing solely on HP3, let's compare the different sentiment dictionaries. AFINN assigns a score from -5 to 5 for each word, while the other dictionaries, bing and nrc, are binary.

```{r 2.3_comparing_sentiment_dictionaries, echo = FALSE}

#isolate hp3
hp3 <- tidy_books %>%
  filter(book == "Prisoner of Azkaban") %>%
  mutate(word_count = 1:n(),
         index = word_count %/% 500 + 1)

#afinn across hp3
afinn <- hp3 %>% 
  inner_join(get_sentiments("afinn")) %>% 
  group_by(index) %>% 
  summarise(sentiment = sum(score)) %>% 
  mutate(method = "AFINN")

#bing and nrc across hp3
bing_and_nrc <- bind_rows(hp3 %>% 
                            inner_join(get_sentiments("bing")) %>%
                            mutate(method = "Bing et al."),
                          hp3 %>% 
                            inner_join(get_sentiments("nrc") %>% 
                                         filter(sentiment %in% c("positive", 
                                                                 "negative"))) %>%
                            mutate(method = "NRC")) %>%
  count(method, index, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)

#comparing the 3 dictionaries
bind_rows(afinn, 
          bing_and_nrc) %>%
  ggplot(aes(index, sentiment, fill = method)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~method, ncol = 1, scales = "free_y")

```

THe overall plot trajectories across the 3 sentiment dictionaries follows the same pattern, although there are some differences. This can be attributed to several things: the ratio of positive/negative words within the dictionaries themselves, as well as how well HP3 in particular matches with the words in each dictionary. 

##Positive vs negative words
After looking at the initial most common negative and positive words, we see that we should add some custom words to the stop_words dataframe. Two of the most negative words are "fudge" and "moody", which, in the context of HP, are character names. 
```{r 2.4_pos_neg_words, echo = FALSE}


#df of words and sentiment using bing dictionary
bing_word_counts<- tidy_books %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()

#plotting most common pos and neg words
bing_word_counts %>%
  group_by(sentiment) %>%
  top_n(10) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales ="free_y") +
  labs(y = "Contribution to sentiment", x = NULL, title = "Most common negative and positive words in HP series") +
  coord_flip()
  
```

##Wordclouds
This wordcloud shows the top 100 most common words in the HP series. Unsurprisingly, the main characters dominate, with "Harry" accounting for 4% of all non-stopword text. 
```{r 2.5_word_cloud, echo = FALSE}

tidy_books_clean %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 100))
  
tidy_books_clean %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("gray20", "gray80"),
                   max.words = 100)

```

##Tokenizing by chapters and sentences
We can tokenize by chapter and sentence, rather than word, to track sentiment across each chapter in HP2.
```{r 2.6_other_tokenizing, echo = FALSE}

#tokenize hp7 only
hp2_sentences <- tibble(chapter = 1:length(chamber_of_secrets),
                        text = chamber_of_secrets) %>% 
  unnest_tokens(sentence, text, token = "sentences")

#df of sentiment and index of each chapter in HP7 ONLY
book_sent <- hp2_sentences %>%
        group_by(chapter) %>%
        mutate(sentence_num = 1:n(),
               index = round(sentence_num / n(), 2)) %>%
        unnest_tokens(word, sentence) %>%
        inner_join(get_sentiments("afinn")) %>%
        group_by(chapter, index) %>%
        summarise(sentiment = sum(score, na.rm = TRUE)) %>%
        arrange(desc(sentiment))

#plotting sentiment throughout the course of hp7
ggplot(book_sent, aes(index, factor(chapter, levels = sort(unique(chapter), decreasing = TRUE)), fill = sentiment)) +
        geom_tile(color = "white") +
        scale_fill_gradient2() +
        scale_x_continuous(labels = scales::percent, expand = c(0, 0)) +
        scale_y_discrete(expand = c(0, 0)) +
        labs(x = "Chapter Progression", y = "Chapter") +
        ggtitle("Sentiment of Harry Potter and the Chamber of Secrets",
                subtitle = "Summary of the net sentiment score as you progress through each chapter") +
        theme_minimal() +
        theme(panel.grid.major = element_blank(),
              panel.grid.minor = element_blank(),
              legend.position = "top")

#finding the most negative chapter of each book

#create df of bing dictionary
bingnegative <- get_sentiments("bing") %>% 
  filter(sentiment == "negative")

#get wordcount of each chapter
wordcounts <- tidy_books %>%
  group_by(book, chapter) %>%
  summarize(words = n())

#find chapter of each book with highest ratio of negative words

print("Chapter in each book with the highest ratio of negative words")
tidy_books %>%
  semi_join(bingnegative) %>%
  group_by(book, chapter) %>%
  summarize(negativewords = n()) %>%
  left_join(wordcounts, by = c("book", "chapter")) %>%
  mutate(ratio = negativewords/words) %>%
  filter(chapter != 0) %>%
  top_n(1) %>%
  ungroup()

```

#Term frequency

##Term frequency in HP series
```{r 3.1_term_freq, echo = FALSE}

#df of each unique word in the books, along with # times it appears
book_words <- tidy_books %>%
  select(-chapter) %>%
  count(book, word, sort = TRUE) %>%
  ungroup()

#total words in each book
total_words <- book_words %>%
  group_by(book) %>%
  summarize(total = sum(n))

book_words <- book_words %>%
  left_join(total_words)

ggplot(book_words, aes(n/total, fill = book)) +
  geom_histogram(show.legend = FALSE) +
  xlim(NA, 0.0009) +
  facet_wrap(~book, ncol = 2, scales = "free_y") +
  ggtitle("Term frequency distribution in HP series")

```

##Zipf's Law
```{r 3.2_zipf, echo = FALSE}

freq_by_rank <- book_words %>% 
  group_by(book) %>% 
  mutate(rank = row_number(), 
         term_frequency = n/total)

freq_by_rank %>%
  ggplot(aes(x = rank, y = term_frequency, color = book)) +
  geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) +
  scale_x_log10() +
  scale_y_log10() +
  ggtitle("Zipf's law in HP series")

freq_by_rank %>% 
  ggplot(aes(rank, term_frequency, color = book)) + 
  geom_abline(intercept = -0.62, slope = -1.1, color = "gray50", linetype = 2) +
  geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) + 
  scale_x_log10() +
  scale_y_log10() +
  ggtitle("Fitting an exponent for Zipf's law within HP series")

```

##Term frequency-inverse document frequency
Idf is 0 for very common words that appear in every book in the series (ln(1) = 0). Idf is higher for words that occur in fewer of the documents in the collection. 

```{r 3.3_tf_itf, echo = FALSE}

book_words <- book_words %>%
  filter(word != "c") %>%
  bind_tf_idf(word, book, n)

book_words %>%
  arrange(desc(tf_idf)) %>%
  mutate(word = factor(word, levels = rev(unique(word)))) %>% 
  group_by(book) %>% 
  top_n(15) %>% 
  ungroup %>%
  ggplot(aes(word, tf_idf, fill = book)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~book, ncol = 2, scales = "free") +
  coord_flip() +
  ggtitle("Words with the highest tf-idf in each book")


```

#Using n-grams to find relationships between words

##Creating bigrams
```{r 4.1_create_bigrams, echo = FALSE}

#Create bigrams from text
hp_bigrams <- tibble()

#for each book, turn into a tidy format (1 row per word) and append to tidy_books table
for(i in seq_along(titles)) {
  
  clean <- tibble(chapter = seq_along(books[[i]]),
                  text = books[[i]]) %>%
    unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
    mutate(book = titles[i]) %>%
    select(book, everything())
  
  hp_bigrams <- rbind(hp_bigrams, clean)
  
  rm(clean)
}

#set factor to keep books in order of publication
hp_bigrams$book <- factor(hp_bigrams$book, levels = rev(titles))

#separate bigrams so that we can remove stopwords
bigrams_separated <- hp_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

# new bigram counts:
bigram_counts <- bigrams_filtered %>% 
  count(word1, word2, sort = TRUE)

#united hp_bigrams with no stopwords
bigrams_united <- bigrams_filtered %>%
  unite(bigram, word1, word2, sep = " ")


#Print the 20 most common bigrams
print("Top 20 most common bigrams (without stopwords)")
bigrams_united %>% 
  count(bigram, sort = TRUE) %>%
  arrange(desc(n)) %>%
  top_n(20)

```

##Analyzing bigrams
We can calculate the tf-idf of bigrams, just like we did for words.
```{r 4.1.2_analyzing_bigrams, echo = FALSE}

bigram_tf_idf <- bigrams_united %>%
  count(book, bigram) %>%
  bind_tf_idf(bigram, book, n) %>%
  arrange(desc(tf_idf))

#plot bigrams with highest tf-idf in each novel
bigram_tf_idf %>%
  arrange(desc(tf_idf)) %>%
  mutate(bigram = factor(bigram, levels = rev(unique(bigram)))) %>% 
  group_by(book) %>% 
  top_n(15) %>% 
  ungroup %>%
  ggplot(aes(bigram, tf_idf, fill = book)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~book, ncol = 2, scales = "free") +
  coord_flip() +
  ggtitle("Bigrams with the highest tf-idf in each book")

```
##Using bigrams to provide context in sentiment analysis
Bigrams can help provide some context in sentiment analysis, since analysis on single words won't take into account negated positive words. We can use bigrams to find the most common of these occurrences ("no good", "never forgive", etc.)
```{r 4.1.3_bigrams_sentiment, echo = FALSE}

negation_words <- c("not", "no", "never", "without")
AFINN <- get_sentiments("afinn")

#bigrams where the first word is "not"
not_words <- bigrams_separated %>%
  filter(word1 == "not") %>%
  inner_join(AFINN, by = c(word2 = "word")) %>%
  count(word2, score, sort = TRUE) %>%
  ungroup()

not_words %>%
  mutate(contribution = n * score) %>%
  arrange(desc(abs(contribution))) %>%
  head(20) %>%
  mutate(word2 = reorder(word2, contribution)) %>%
  ggplot(aes(word2, n * score, fill = n * score > 0)) +
  geom_col(show.legend = FALSE) +
  xlab("Words preceded by \"not\"") +
  ylab("Sentiment score * number of occurrences") +
  coord_flip() +
  ggtitle("Most common positive and negative words preceded by \"not\"")

#bigrams where the first word is one of 4 negation words
negated_words <- bigrams_separated %>%
  filter(word1 %in% negation_words) %>%
  inner_join(AFINN, by = c(word2 = "word")) %>%
  count(word1, word2, score, sort = TRUE) %>%
  ungroup()

negated_words %>%
  group_by(word1) %>%
  arrange(desc(n)) %>%
  filter(row_number() <= 20) %>%
  ungroup() %>%
  mutate(contribution = n * score) %>%
  mutate(direction = ifelse(contribution >= 0, "positive", "negative")) %>%
  ggplot(aes(x = reorder(word2,contribution), y = contribution, fill = factor(direction))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~word1, scales = "free") +
  labs(y = "AFINN score * n",
       x = "Top 10 words associated with negation word") +
  coord_flip() +
  ggtitle("Most common positive and negative words preceded by negation word")


```
##Graphing bigrams using igraph and ggraph
We can use igraph and ggraph to visualize how common bigrams (those that occur >20 times) are related.
```{r 4.1.4_graphing_bigrams, echo = FALSE, height = 9, width = 10}

#df of bigram counts
bigram_counts <- bigrams_filtered %>% 
  count(word1, word2, sort = TRUE)

# filter for only relatively common combinations
bigram_graph <- bigram_counts %>%
  filter(n > 30) %>%
  graph_from_data_frame()

#use ggraph to graph the bigram_graph
set.seed(2017)

ggraph(bigram_graph, layout = "fr") +
  geom_edge_link() +
  geom_node_point() +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1)

a <- grid::arrow(type = "closed", length = unit(.05, "inches"))

ggraph(bigram_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = a, end_cap = circle(.05, 'inches')) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()

```
##Counting and correlating pairs

```{r 4.2_correlation, echo = FALSE}

#divide up HP4 into 100 word chunks, removing "'s"
hp_section_words <- tidy_books_clean %>%
  filter(book == "Goblet of Fire") %>%
  mutate(word = gsub("'s","", word)) %>%
  group_by(book) %>% 
  mutate(word_count = 1:n(),
         index = word_count %/% 100 + 1) %>%
  select(-chapter, -word_count)

#count pairs that tend to occur in same section
word_pairs <- hp_section_words %>%
  pairwise_count(word, index, sort = TRUE)

print("Top 20 words that appear most ofted with \"harry\" within each 100 word chunk in HP4")
word_pairs %>%
  filter(item1 == "harry") %>%
  group_by(item1, item2) %>%
  summarize(total = sum(n)) %>%
  arrange(desc(total)) %>%
  top_n(20)

#calculate word correlations
word_cors <- hp_section_words %>%
  group_by(word) %>%
  filter(n() >= 30) %>%
  pairwise_cor(word, index, sort = TRUE)

#plotting most correlated words in HP4
word_cors %>%
  filter(item1 %in% c("harry", "malfoy", "hermione", "voldemort")) %>%
  group_by(item1) %>%
  top_n(10) %>%
  ungroup() %>%
  mutate(item2 = reorder(item2, correlation)) %>%
  ggplot(aes(item2, correlation, fill = item1)) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  facet_wrap(~ item1, scales = "free") +
  coord_flip() + 
  ggtitle("Top 10 most correlated words within each 100 word chunk in HP4")

#graphing word correlations
set.seed(2016)

word_cors %>%
  filter(correlation > .3) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link() +
  geom_node_point() +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1)

```

#Topic modeling

##Topic modeling examples using LDA
In order to to LDA topic modeling, we need to first convert the tidy dataset into a document-term-matrix format, something that is included in the tidytext package. After performing the lda, we convert the data back into a tidy df.
```{r 6.1.1_lda, echo = FALSE}

#convert book_words df into a document term matrix
book_words_dtm <- book_words %>%
  #remove stopwords 
  anti_join(stop_words) %>%
  #remove the columns we created for tf, idf, total
  select(book, word, n) %>%
  cast_dtm(book, word, n)

#set a seed so that the output of the model is predictable
hp_lda <- LDA(book_words_dtm, k = 2, control = list(seed = 1234))

#tidying the LDA matrix
hp_topics <- tidy(hp_lda, matrix = "beta")

#top terms in each topic
top_terms <- hp_topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

#graphing the top terms
top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip() +
  ggtitle("Most common terms in each topic")

```

##Beta
```{r 6.1.1_word_topic_prob, echo = FALSE}

#finding terms with the greatest difference in beta between topics 1 and 2
beta_spread <- hp_topics %>%
  mutate(topic = paste0("topic", topic)) %>%
  spread(topic, beta) %>%
  filter(topic1 > .001 | topic2 > .001) %>%
  mutate(log_ratio = log2(topic2 / topic1))

#graphing the terms with largest difference in beta
#largest positive beta
beta_spread_max <- 
  beta_spread %>%
  top_n(10, log_ratio)

#largest negative beta
beta_spread_min <- 
  beta_spread %>%
  top_n(-10, log_ratio)

#combine largest pos and neg together
beta_spread_min_max <- bind_rows(beta_spread_max, beta_spread_min) %>%
  mutate(direction = ifelse(log_ratio >= 0, "positive", "negative"))
  
beta_spread_min_max %>%
  mutate(term = reorder(term, log_ratio)) %>%
  ggplot(aes(term, log_ratio, fill = factor(direction))) +
  geom_col(show.legend = FALSE) +
  #facet_wrap(~ topic, scales = "free") +
  coord_flip() +
  ggtitle("Terms with largest log2 ratio of betas (pos and neg) between topics 2/1")

```

##Document-topic probabilities
LDA models each document as a mixture of topics. However, when looking at the table of book topics, we see that all of the books are roughly distributed equally between topics 1 and 2. This isn't surprising given the top terms in each of the topics. 
```{r 6.1.2_doc_topic_prob, echo = FALSE}

#tidying the LDA matrix
hp_book_topics <- tidy(hp_lda, matrix = "gamma")

hp_book_topics

```

##Doing LDA on chapters
Performing LDA on individual chapters of HP books. 

This first time, we see that some common terms "harry", "ron", "hermione" etc appear in every topic. 
```{r 6.2.1_lda_chapters, echo = FALSE}

#calculate the number of times each word appears within a chapter
word_counts <- tidy_books_clean %>%
  mutate(book_chapter = paste(book, chapter, sep = "_")) %>%
  select(-book, -chapter) %>%
  count(book_chapter, word)

#cast as document term matrix
chapters_dtm <- word_counts %>%
  cast_dtm(book_chapter, word, n)

#try to create a 7 topic model (since there are 7 books)
chapters_lda <- LDA(chapters_dtm, k = 7, control = list(seed = 1234))

#turn back into a tidy df
chapters_topics <- tidy(chapters_lda, matrix = "beta")

#top terms in each of the 7 topics
top_terms <- chapters_topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

#graph the top terms
top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()
```


##Per-document classification
```{r 6.2.2_per_document_classification, echo = FALSE}
chapters_gamma <- tidy(chapters_lda, matrix = "gamma")

chapters_gamma <- chapters_gamma %>%
  separate(document, c("title", "chapter"), sep = "_", convert = TRUE)

# reorder titles in order of topic 1, topic 2, etc before plotting
chapters_gamma %>%
  mutate(title = reorder(title, gamma * topic)) %>%
  ggplot(aes(factor(topic), gamma)) +
  geom_boxplot() +
  facet_wrap(~ title) +
  ggtitle("Gamma probabilities for each chapter within each book")

```
##By-word assignments
```{r 6.2.3_by_word_assignments, echo = FALSE}

#assigning each chapter to its top topic match
chapter_classifications <- chapters_gamma %>%
  group_by(title, chapter) %>%
  top_n(1, gamma) %>%
  ungroup()

#compare "consensus" topics for each book (most common topic amongst chapters) and compare to chapter topic
book_topics <- chapter_classifications %>%
  count(title, topic) %>%
  group_by(title) %>%
  top_n(1, n) %>%
  ungroup() %>%
  transmute(consensus = title, topic)

chapter_classifications %>%
  inner_join(book_topics, by = "topic") %>%
  filter(title != consensus)

assignments <- augment(chapters_lda, data = chapters_dtm)

assignments <- assignments %>%
  separate(document, c("title", "chapter"), sep = "_", convert = TRUE) %>%
  inner_join(book_topics, by = c(".topic" = "topic"))

assignments %>%
  count(title, consensus, wt = count) %>%
  group_by(title) %>%
  mutate(percent = n / sum(n)) %>%
  ggplot(aes(consensus, title, fill = percent)) +
  geom_tile() +
  scale_fill_gradient2(high = "red", label = percent_format()) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1),
        panel.grid = element_blank()) +
  labs(x = "Book words were assigned to",
       y = "Book words came from",
       fill = "% of assignments") +
  ggtitle("Confusion matrix showing where LDA assigned words from each book")

```
```{r example, echo = FALSE}


```
